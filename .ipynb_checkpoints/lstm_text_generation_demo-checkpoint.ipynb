{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this tutorial we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland and then use the model to generate new sequences of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ascii text and covert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to numerical value(integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the list of unique sorted lowercase characters in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some characters that we could remove to further clean up the dataset that will reduce the vocabulary \n",
    "and may improve the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, '[': 29, ']': 30, '_': 31, 'a': 32, 'b': 33, 'c': 34, 'd': 35, 'e': 36, 'f': 37, 'g': 38, 'h': 39, 'i': 40, 'j': 41, 'k': 42, 'l': 43, 'm': 44, 'n': 45, 'o': 46, 'p': 47, 'q': 48, 'r': 49, 's': 50, 't': 51, 'u': 52, 'v': 53, 'w': 54, 'x': 55, 'y': 56, 'z': 57}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the details of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are just under 150,000 characters and that when converted to lowercase that there are only 47 distinct characters in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163780\n",
      "Total Vocab:  58\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define how we train the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. You can change the length of characters as you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training pattern of the network is comprised of 100 characters (X) followed by one character output (y). Then using the sliding window mechanism, we slide along the whole book one character at a time. Suppose if we have a sequence of length 5 (CHAPTER), then first two training instances would be:\n",
    "\n",
    "CHAPT -> E<br>\n",
    "HAPTE -> R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163680\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100 #can be changed\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 49, 46, 41, 36, 34, 51, 1, 38, 52, 51, 36, 45, 33, 36, 49, 38, 7, 50, 1, 32, 43, 40, 34, 36, 7, 50, 1, 32, 35, 53, 36, 45, 51, 52, 49, 36, 50, 1, 40, 45, 1, 54, 46, 45, 35, 36, 49, 43, 32, 45, 35, 11, 1, 33, 56, 1, 43, 36, 54, 40, 50, 1, 34, 32, 49, 49, 46, 43, 43, 0, 0, 51, 39, 40, 50, 1, 36, 33, 46, 46, 42, 1, 40, 50, 1, 37, 46, 49, 1, 51, 39, 36, 1, 52, 50, 36, 1, 46, 37]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# checking dataX and dataY\n",
    "print(dataX[0])\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, time steps, features] that is expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range [0,1] to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output values (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary given the input sequence. Each y value is converted into a sparse vector with a length of 47, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, when “n” (integer value 31) is one hot encoded it looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize - rescaling the integer values\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is defined as single character classification problem with 47 classes. So, we use cross entropy as loss function and ADAM as the optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> We use the dropout to obtain generalization of the dataset instead of overfitting the training dataset perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]))) #It can have 1 or more training samples\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network might be slow to train (if not using GPU). Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to generate the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These hyperparameters can be configured as required\n",
    "epochs = 10\n",
    "batch_size = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.9772\n",
      "Epoch 00001: loss improved from inf to 2.97719, saving model to weights-improvement-01-2.9772.hdf5\n",
      "1279/1279 [==============================] - 341s 267ms/step - loss: 2.9772\n",
      "Epoch 2/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.7916\n",
      "Epoch 00002: loss improved from 2.97719 to 2.79160, saving model to weights-improvement-02-2.7916.hdf5\n",
      "1279/1279 [==============================] - 329s 257ms/step - loss: 2.7916\n",
      "Epoch 3/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.7019\n",
      "Epoch 00003: loss improved from 2.79160 to 2.70187, saving model to weights-improvement-03-2.7019.hdf5\n",
      "1279/1279 [==============================] - 336s 263ms/step - loss: 2.7019\n",
      "Epoch 4/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.6334\n",
      "Epoch 00004: loss improved from 2.70187 to 2.63345, saving model to weights-improvement-04-2.6334.hdf5\n",
      "1279/1279 [==============================] - 351s 274ms/step - loss: 2.6334\n",
      "Epoch 5/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.5811\n",
      "Epoch 00005: loss improved from 2.63345 to 2.58111, saving model to weights-improvement-05-2.5811.hdf5\n",
      "1279/1279 [==============================] - 335s 262ms/step - loss: 2.5811\n",
      "Epoch 6/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.5294\n",
      "Epoch 00006: loss improved from 2.58111 to 2.52943, saving model to weights-improvement-06-2.5294.hdf5\n",
      "1279/1279 [==============================] - 334s 261ms/step - loss: 2.5294\n",
      "Epoch 7/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.4795\n",
      "Epoch 00007: loss improved from 2.52943 to 2.47952, saving model to weights-improvement-07-2.4795.hdf5\n",
      "1279/1279 [==============================] - 329s 257ms/step - loss: 2.4795\n",
      "Epoch 8/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.4339\n",
      "Epoch 00008: loss improved from 2.47952 to 2.43389, saving model to weights-improvement-08-2.4339.hdf5\n",
      "1279/1279 [==============================] - 330s 258ms/step - loss: 2.4339\n",
      "Epoch 9/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.3927\n",
      "Epoch 00009: loss improved from 2.43389 to 2.39267, saving model to weights-improvement-09-2.3927.hdf5\n",
      "1279/1279 [==============================] - 330s 258ms/step - loss: 2.3927\n",
      "Epoch 10/10\n",
      "1279/1279 [==============================] - ETA: 0s - loss: 2.3517\n",
      "Epoch 00010: loss improved from 2.39267 to 2.35174, saving model to weights-improvement-10-2.3517.hdf5\n",
      "1279/1279 [==============================] - 329s 257ms/step - loss: 2.3517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff6b42f97b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we will have a number of weight checkpoint files in the local directory. For the next step, we can take the weights with smallest loss value: <b> weights-improvement-10-2.3517.hdf5 </b>\n",
    "\n",
    "Also, we can notice that the loss is decreasing after every epoch, so training for larger epochs will better fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text with trained LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the network in exactly the same way it was defined while training. Then we can load the network weights from the checkpoint file. In this way, we don't have to train the network(model) again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-10-2.3517.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we prepared the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can make the predictions.\n",
    "\n",
    "To start off, we need to have a seed sequence as an input to the model. Passing the random seed sequence to the model will generate or predict the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "Seed:\n",
      "\" yone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  you may copy it, give it away  \"\n"
     ]
    }
   ],
   "source": [
    "# generate a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "print(start)\n",
    "pattern = dataX[start] #dataX contains list of patterns\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'n', ' ', 'a', 'n', 'l', ' ', 'a', 'o', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'i', 'n', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'a', 'r', 'o', ' ', 'o', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'r', 'e', 'e', ' ', 't', 'h', ' ', 't', 'h', 'e', ' ', 's', 'o', 'e', 'e', 'e', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'c', 'a', 'r', 'e', ' ', 'a', 'n']\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "length = 100\n",
    "final = []\n",
    "for i in range(length):\n",
    "    # reshaping the seed sequence before passing it into the LSTM model\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    # normalizing the ineger values\n",
    "    x = x / float(n_vocab)\n",
    "    # making prediction\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    # Get the predicted value with maximum probability\n",
    "    index = numpy.argmax(prediction)\n",
    "    # Convert the predicted integer to char\n",
    "    result = int_to_char[index]\n",
    "    final.append(result)\n",
    "    # Adding the predicted character to the sequence sequence\n",
    "    pattern.append(index)\n",
    "    # Removing the first character from the seed sequence\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm-demo",
   "language": "python",
   "name": "lstm-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
